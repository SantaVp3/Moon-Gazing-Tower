package scanner

import (
	"crypto/tls"
	"fmt"
	"io"
	"net/http"
	"path/filepath"
	"sync"
	"time"

	"github.com/reconmaster/backend/internal/models"
)

// SiteScanner ç«™ç‚¹æ‰«æå™¨
type SiteScanner struct {
	client      *http.Client
	crawler     *Crawler
	fingerprint bool
}

// NewSiteScanner åˆ›å»ºç«™ç‚¹æ‰«æå™¨
func NewSiteScanner() *SiteScanner {
	// åˆå§‹åŒ–æŒ‡çº¹åº“
	InitFingerprints()

	return &SiteScanner{
		client: &http.Client{
			Timeout: 5 * time.Second, // ä¼˜åŒ–ï¼šé™ä½HTTPè¶…æ—¶æ—¶é—´åˆ°5ç§’
			Transport: &http.Transport{
				TLSClientConfig:     &tls.Config{InsecureSkipVerify: true},
				MaxIdleConns:        100,              // ä¼˜åŒ–ï¼šå¢åŠ è¿æ¥æ± 
				MaxIdleConnsPerHost: 10,               // ä¼˜åŒ–ï¼šå¢åŠ æ¯ä¸ªhostçš„è¿æ¥æ•°
				IdleConnTimeout:     30 * time.Second, // ä¼˜åŒ–ï¼šè¿æ¥å¤ç”¨
			},
			CheckRedirect: func(req *http.Request, via []*http.Request) error {
				if len(via) >= 5 {
					return http.ErrUseLastResponse
				}
				return nil
			},
		},
		crawler:     NewCrawler(),
		fingerprint: true,
	}
}

// Detect è¯†åˆ«ç«™ç‚¹
func (ss *SiteScanner) Detect(ctx *ScanContext) error {
	// ğŸ†• åŠ è½½æ‰«æå™¨é…ç½®
	scannerConfig := LoadScannerConfig(ctx)
	ss.client.Timeout = scannerConfig.SiteTimeout

	// ğŸ†• ä½¿ç”¨é…ç½®é‡æ–°åˆ›å»ºçˆ¬è™«
	ss.crawler = NewCrawlerWithConfig(CrawlerConfig{
		MaxDepth: scannerConfig.CrawlerMaxDepth,
		MaxPages: scannerConfig.CrawlerMaxPages,
		Timeout:  scannerConfig.SiteTimeout,
	})

	var ports []models.Port
	ctx.DB.Where("task_id = ? AND port IN (?)", ctx.Task.ID, []int{80, 443, 8080, 8443, 8888, 8000, 8001, 9090}).Find(&ports)

	ctx.Logger.Printf("Detecting sites for %d ports", len(ports))

	// ğŸ†• ä½¿ç”¨é…ç½®çš„å¹¶å‘æ•°
	concurrency := scannerConfig.SiteConcurrency
	if len(ports) < concurrency {
		concurrency = len(ports)
	}
	ctx.Logger.Printf("[Config] Site scanner: concurrency=%d, timeout=%v, crawler_depth=%d, crawler_pages=%d",
		concurrency, ss.client.Timeout, scannerConfig.CrawlerMaxDepth, scannerConfig.CrawlerMaxPages)

	var wg sync.WaitGroup
	semaphore := make(chan struct{}, concurrency)

	for _, port := range ports {
		// æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¢«å–æ¶ˆ
		select {
		case <-ctx.Ctx.Done():
			ctx.Logger.Printf("Site detection cancelled by user")
			wg.Wait()
			return ctx.Ctx.Err()
		default:
		}

		wg.Add(1)
		go func(p models.Port) {
			defer wg.Done()
			semaphore <- struct{}{}
			defer func() { <-semaphore }()

			// æ£€æŸ¥å–æ¶ˆçŠ¶æ€
			select {
			case <-ctx.Ctx.Done():
				return
			default:
			}

			ss.detectSiteForPort(ctx, p)
		}(port)
	}

	wg.Wait()
	ctx.Logger.Printf("Site detection completed")
	return nil
}

// detectSiteForPort æ£€æµ‹å•ä¸ªç«¯å£çš„ç«™ç‚¹
func (ss *SiteScanner) detectSiteForPort(ctx *ScanContext, port models.Port) {
	schemes := []string{"http"}
	if port.Port == 443 || port.Port == 8443 {
		schemes = []string{"https"}
	} else if port.Port == 80 || port.Port == 8080 || port.Port == 8888 {
		// å°è¯•ä¸¤ç§åè®®
		schemes = []string{"http", "https"}
	}

	// æŸ¥æ‰¾è¯¥IPå¯¹åº”çš„åŸŸå
	var domains []models.Domain
	ctx.DB.Where("task_id = ? AND ip_address = ?", ctx.Task.ID, port.IPAddress).Find(&domains)

	// ä¼˜å…ˆä½¿ç”¨åŸŸåï¼Œå¦‚æœæ²¡æœ‰åŸŸååˆ™ä½¿ç”¨IP
	hosts := make([]string, 0)
	for _, domain := range domains {
		hosts = append(hosts, domain.Domain)
	}
	if len(hosts) == 0 {
		hosts = append(hosts, port.IPAddress)
	}

	for _, host := range hosts {
		for _, scheme := range schemes {
			url := fmt.Sprintf("%s://%s:%d", scheme, host, port.Port)

			if siteInfo := ss.probeSite(url); siteInfo != nil {
				siteInfo.TaskID = ctx.Task.ID
				ctx.DB.Create(siteInfo)
				ctx.Logger.Printf("Site detected: %s - %s", url, siteInfo.Title)

				// å¦‚æœå¯ç”¨äº†çˆ¬è™«
				if ctx.Task.Options.EnableCrawler {
					ss.crawlSite(ctx, url)
				}

				break // æˆåŠŸåä¸å†å°è¯•å…¶ä»–åè®®
			}
		}
	}
}

// probeSite æ¢æµ‹ç«™ç‚¹
func (ss *SiteScanner) probeSite(url string) *models.Site {
	req, err := http.NewRequest("GET", url, nil)
	if err != nil {
		return nil
	}

	// è®¾ç½®User-Agent
	req.Header.Set("User-Agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")

	resp, err := ss.client.Do(req)
	if err != nil {
		return nil
	}
	defer resp.Body.Close()

	// è¯»å–bodyç”¨äºæŒ‡çº¹è¯†åˆ«
	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil
	}
	bodyStr := string(body)

	// æå–æ ‡é¢˜
	title := ExtractTitle(bodyStr)

	// è·å–æ‰€æœ‰headers
	headers := make(map[string]string)
	for k, v := range resp.Header {
		if len(v) > 0 {
			headers[k] = v[0]
		}
	}

	// æŒ‡çº¹è¯†åˆ«
	fingerprints := MatchFingerprints(headers, bodyStr, title)

	// åˆå¹¶æŒ‡çº¹ä¸ºå­—ç¬¦ä¸²
	fingerprintStr := ""
	if len(fingerprints) > 0 {
		fingerprintStr = fingerprints[0]
		for i := 1; i < len(fingerprints) && i < 5; i++ { // æœ€å¤šæ˜¾ç¤º5ä¸ª
			fingerprintStr += ", " + fingerprints[i]
		}
	}

	// CDNæ£€æµ‹
	isCDN := IsCDN(headers, "")

	// ä»URLä¸­æå–IP
	ip := ExtractIPFromURL(url)

	site := &models.Site{
		URL:          url,
		StatusCode:   resp.StatusCode,
		IP:           ip, // æ·»åŠ IP
		ContentType:  resp.Header.Get("Content-Type"),
		Server:       resp.Header.Get("Server"),
		Title:        title,
		Fingerprint:  fingerprintStr, // æ·»åŠ å•ä¸ªæŒ‡çº¹å­—ç¬¦ä¸²
		Fingerprints: fingerprints,   // ä¿ç•™æ•°ç»„
	}

	// è®°å½•CDNä¿¡æ¯åˆ°Serverå­—æ®µ
	if isCDN {
		site.Server += " [CDN]"
	}

	return site
}

// crawlSite çˆ¬å–ç«™ç‚¹
func (ss *SiteScanner) crawlSite(ctx *ScanContext, url string) {
	ctx.Logger.Printf("Starting crawler for site: %s", url)

	// ä½¿ç”¨ä»»åŠ¡é€‰é¡¹é…ç½®çˆ¬è™«
	config := CrawlerConfig{
		MaxDepth: 3,
		MaxPages: 100,
		Timeout:  10 * time.Second,
	}

	// å¦‚æœä»»åŠ¡é€‰é¡¹ä¸­æœ‰çˆ¬è™«é…ç½®ï¼Œä½¿ç”¨å®ƒä»¬
	if ctx.Task.Options.CrawlerDepth > 0 {
		config.MaxDepth = ctx.Task.Options.CrawlerDepth
	}
	if ctx.Task.Options.CrawlerPages > 0 {
		config.MaxPages = ctx.Task.Options.CrawlerPages
	}

	crawler := NewCrawlerWithConfig(config)
	err := crawler.Crawl(ctx, url)
	if err != nil {
		ctx.Logger.Printf("[Crawler] Failed for %s: %v", url, err)
		return
	}

	ctx.Logger.Printf("[Crawler] Completed for %s", url)
}

// TakeScreenshots ç«™ç‚¹æˆªå›¾
func (ss *SiteScanner) TakeScreenshots(ctx *ScanContext) error {
	var sites []models.Site
	if err := ctx.DB.Where("task_id = ?", ctx.Task.ID).Find(&sites).Error; err != nil {
		return fmt.Errorf("failed to fetch sites: %w", err)
	}

	if len(sites) == 0 {
		ctx.Logger.Printf("No sites to screenshot")
		return nil
	}

	ctx.Logger.Printf("Taking screenshots for %d sites", len(sites))

	// åˆ›å»ºæˆªå›¾æ‰«æå™¨
	screenshotDir := "./data/screenshots"
	screenshotScanner := NewScreenshotScanner(screenshotDir)

	// å¹¶å‘æˆªå›¾ï¼ˆé™åˆ¶å¹¶å‘æ•°ä¸º3ï¼Œé¿å…èµ„æºå ç”¨è¿‡é«˜ï¼‰
	concurrency := 3
	semaphore := make(chan struct{}, concurrency)
	var wg sync.WaitGroup

	successCount := 0
	failCount := 0
	var mu sync.Mutex

	for _, site := range sites {
		wg.Add(1)
		go func(s models.Site) {
			defer wg.Done()
			semaphore <- struct{}{}
			defer func() { <-semaphore }()

			ctx.Logger.Printf("Taking screenshot: %s", s.URL)

			// ä½¿ç”¨å¯è§†åŒºåŸŸæˆªå›¾ï¼ˆæ›´å¿«ï¼‰
			screenshotPath, err := screenshotScanner.ScreenshotViewport(s.URL)
			if err != nil {
				ctx.Logger.Printf("Screenshot failed for %s: %v", s.URL, err)
				mu.Lock()
				failCount++
				mu.Unlock()
				return
			}

			// åªä¿å­˜æ–‡ä»¶åï¼Œä¸ä¿å­˜å®Œæ•´è·¯å¾„
			filename := filepath.Base(screenshotPath)

			// æ›´æ–°æ•°æ®åº“ä¸­çš„æˆªå›¾è·¯å¾„
			if err := ctx.DB.Model(&models.Site{}).
				Where("id = ?", s.ID).
				Update("screenshot", filename).Error; err != nil {
				ctx.Logger.Printf("Failed to update screenshot path for %s: %v", s.URL, err)
				mu.Lock()
				failCount++
				mu.Unlock()
				return
			}

			ctx.Logger.Printf("Screenshot saved: %s -> %s", s.URL, filename)
			mu.Lock()
			successCount++
			mu.Unlock()
		}(site)
	}

	wg.Wait()

	ctx.Logger.Printf("Screenshot completed: %d success, %d failed", successCount, failCount)
	return nil
}

// CheckFileLeaks æ–‡ä»¶æ³„éœ²æ£€æµ‹
func (ss *SiteScanner) CheckFileLeaks(ctx *ScanContext) error {
	var sites []models.Site
	ctx.DB.Where("task_id = ?", ctx.Task.ID).Find(&sites)

	ctx.Logger.Printf("Checking file leaks for %d sites", len(sites))

	// æ•æ„Ÿæ–‡ä»¶è·¯å¾„
	leakPaths := []struct {
		path     string
		severity string
		desc     string
	}{
		{"/.git/config", "high", "Gité…ç½®æ–‡ä»¶æ³„éœ²"},
		{"/.git/HEAD", "high", "Gitä»“åº“æ³„éœ²"},
		{"/.env", "critical", "ç¯å¢ƒå˜é‡æ–‡ä»¶æ³„éœ²"},
		{"/.env.local", "high", "æœ¬åœ°ç¯å¢ƒé…ç½®æ³„éœ²"},
		{"/.env.production", "high", "ç”Ÿäº§ç¯å¢ƒé…ç½®æ³„éœ²"},
		{"/web.config", "medium", "IISé…ç½®æ–‡ä»¶æ³„éœ²"},
		{"/.DS_Store", "low", "Macç³»ç»Ÿæ–‡ä»¶æ³„éœ²"},
		{"/backup.zip", "high", "å¤‡ä»½æ–‡ä»¶æ³„éœ²"},
		{"/backup.tar.gz", "high", "å¤‡ä»½æ–‡ä»¶æ³„éœ²"},
		{"/backup.sql", "critical", "æ•°æ®åº“å¤‡ä»½æ³„éœ²"},
		{"/db.sql", "critical", "æ•°æ®åº“æ–‡ä»¶æ³„éœ²"},
		{"/database.sql", "critical", "æ•°æ®åº“æ–‡ä»¶æ³„éœ²"},
		{"/.svn/entries", "high", "SVNä¿¡æ¯æ³„éœ²"},
		{"/phpinfo.php", "medium", "PHPä¿¡æ¯æ³„éœ²"},
		{"/info.php", "medium", "PHPä¿¡æ¯æ³„éœ²"},
		{"/test.php", "low", "æµ‹è¯•æ–‡ä»¶æ³„éœ²"},
		{"/config.php", "high", "é…ç½®æ–‡ä»¶æ³„éœ²"},
		{"/config.json", "high", "é…ç½®æ–‡ä»¶æ³„éœ²"},
		{"/config.yml", "high", "é…ç½®æ–‡ä»¶æ³„éœ²"},
		{"/config.yaml", "high", "é…ç½®æ–‡ä»¶æ³„éœ²"},
		{"/settings.py", "high", "Djangoé…ç½®æ³„éœ²"},
		{"/application.properties", "high", "Springé…ç½®æ³„éœ²"},
		{"/application.yml", "high", "Springé…ç½®æ³„éœ²"},
		{"/.htaccess", "medium", "Apacheé…ç½®æ³„éœ²"},
		{"/robots.txt", "info", "Robotsæ–‡ä»¶"},
		{"/sitemap.xml", "info", "ç«™ç‚¹åœ°å›¾"},
		{"/README.md", "low", "READMEæ–‡ä»¶æ³„éœ²"},
		{"/CHANGELOG.md", "low", "å˜æ›´æ—¥å¿—æ³„éœ²"},
	}

	for _, site := range sites {
		for _, leak := range leakPaths {
			url := site.URL + leak.path

			statusCode, contentType, size := ss.checkURLDetailed(url)
			if statusCode == 200 && size > 0 {
				vuln := &models.Vulnerability{
					TaskID:      ctx.Task.ID,
					URL:         url,
					Type:        "file_leak",
					Severity:    leak.severity,
					Title:       leak.desc,
					Description: fmt.Sprintf("å‘ç°æ•æ„Ÿæ–‡ä»¶: %s (å¤§å°: %d bytes, ç±»å‹: %s)", url, size, contentType),
					Solution:    "åˆ é™¤æˆ–é™åˆ¶å¯¹æ•æ„Ÿæ–‡ä»¶çš„è®¿é—®",
				}
				ctx.DB.Create(vuln)
				ctx.Logger.Printf("File leak found: %s [%s]", url, leak.severity)
			}
		}
	}

	return nil
}

// checkURLDetailed è¯¦ç»†æ£€æŸ¥URL
func (ss *SiteScanner) checkURLDetailed(url string) (int, string, int64) {
	req, err := http.NewRequest("GET", url, nil)
	if err != nil {
		return 0, "", 0
	}

	req.Header.Set("User-Agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")

	resp, err := ss.client.Do(req)
	if err != nil {
		return 0, "", 0
	}
	defer resp.Body.Close()

	// è¯»å–bodyè·å–å¤§å°
	body, _ := io.ReadAll(resp.Body)

	return resp.StatusCode, resp.Header.Get("Content-Type"), int64(len(body))
}

// RunNuclei å·²åºŸå¼ƒ - ä½¿ç”¨æ™ºèƒ½PoCæ£€æµ‹æ›¿ä»£
func (ss *SiteScanner) RunNuclei(ctx *ScanContext) error {
	ctx.Logger.Printf("RunNuclei is deprecated, use smart PoC detection instead")
	return nil
}
